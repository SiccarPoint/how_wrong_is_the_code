# Dakota input file, uses a moving average approach with a finer scale,
# combined with a second calibration term based on the std of bug find
# rates when at low commits (when there is much scatter, and we know N0
# controls this scatter), i.e., the first exploits the shape and the
# second exploits the distribution of the response.

# This version iterates on 3 by asking for a more discretised fit to scatter
# across the sampled space

# This attempts a nonlinear least squares Dakota method on the data to try to
# get confidence intervals
# see https://dakota.sandia.gov/sites/default/files/docs/6.10/html-ref/method-nl2sol.html

environment
  tabular_data
    tabular_data_file "dakota_nl2sol1.dat"

method
  max_iterations = 100
  nl2sol

model
  single

variables
  continuous_design 3
  descriptors "R" "S" "F"
  lower_bounds 0.001 0.001 0.001
  upper_bounds 0.25 1 0.25
  initial_point 0.1 0.5 0.1

interface
  fork
    asynchronous
    evaluation_concurrency 4
    parameters_file "params.in"
    results_file "results.out"
    copy_files "template_dir/*"

    analysis_driver "python model_command_line_driver_nl2sol1.py"

    work_directory
      named "NL2SOL1/run"
      directory_tag
      directory_save
      file_save

responses
  calibration_terms 2
  response_descriptors "rmse" "rmse_std"  # units both just bfr
  numerical_gradients
    method_source dakota
    interval_type forward  # central
  no_hessians
